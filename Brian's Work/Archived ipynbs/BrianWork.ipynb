{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4686f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc26df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML loaded successfully\n",
      "Number of patch panels found: 4\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\renai\\OneDrive\\College\\Info 519 NLP and Deep Learning\\Group Project\\Project Files\\Diablo_IV_Patch_Notes.html\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "print(\"HTML loaded successfully\")\n",
    "\n",
    "panels = soup.find_all(\"div\", class_=\"panel\")\n",
    "print(\"Number of patch panels found:\", len(panels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3d3413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted patches: 4\n",
      "\n",
      "First patch header:\n",
      " 2.5.3 Build #70356 (All Platforms)—January 28, 2026\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "patch_data = []\n",
    "\n",
    "for panel in panels:\n",
    "    \n",
    "    # Extract header text (version + build + date)\n",
    "    header = panel.find(\"a\")\n",
    "    header_text = header.get_text(strip=True) if header else \"Unknown\"\n",
    "    \n",
    "    # Extract patch body\n",
    "    body_div = panel.find(\"div\", class_=\"panel-body\")\n",
    "    body_text = body_div.get_text(separator=\"\\n\", strip=True) if body_div else \"\"\n",
    "    \n",
    "    patch_data.append({\n",
    "        \"header\": header_text,\n",
    "        \"content\": body_text\n",
    "    })\n",
    "\n",
    "print(\"Extracted patches:\", len(patch_data))\n",
    "print(\"\\nFirst patch header:\\n\", patch_data[0][\"header\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3a72ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': '2.5.3', 'build': '70356', 'date': 'January 28, 2026', 'content': \"Bug Fixes\\nFixed an issue where the Executioner Monster Affix sound effect played continuously.\\nDeveloper’s Note:\\nThis affix will be re-enabled with the release of this patch.\\nFixed an issue where certain Silent Chests in Nahantu did not count towards the Season Rank objective Test of Luck.\\nFixed an issue where Zagraal in the Dark Citadel didn't drop loot.\\nFixed an issue where some Tower bosses had significantly more health than others.\\nFixed an issue where an error would occur when trying to view the profile of a leaderboard entry and the player had a private profile.\\nFixed an issue where resetting a piece of masterworked gear did not unlock the Recycled Works challenge.\\nFixed an issue where the reward for defeating all the Lesser Evils at once was only given to the player who opened the chest in Local Co-op play.\\nFixed an issue where other Divine Gifts could stop gaining experience if another Divine Gift was at Rank 5.\\nFixed an issue where some cosmetic items would show as locked even though they were unlocked and usable.\\nFixed various instances of Placeholder assets in the Wardrobe.\\nVarious performance, stability, and audio improvements.\"}\n"
     ]
    }
   ],
   "source": [
    "structured_patches = []\n",
    "\n",
    "for patch in patch_data:\n",
    "    header = patch[\"header\"]\n",
    "    \n",
    "    # Extract version\n",
    "    version_match = re.search(r\"(\\d+\\.\\d+\\.\\d+)\", header)\n",
    "    version = version_match.group(1) if version_match else None\n",
    "    \n",
    "    # Extract build number\n",
    "    build_match = re.search(r\"Build\\s+#(\\d+)\", header)\n",
    "    build = build_match.group(1) if build_match else None\n",
    "    \n",
    "    # Extract date\n",
    "    date_match = re.search(r\"—(.+)\", header)\n",
    "    date = date_match.group(1).strip() if date_match else None\n",
    "    \n",
    "    structured_patches.append({\n",
    "        \"version\": version,\n",
    "        \"build\": build,\n",
    "        \"date\": date,\n",
    "        \"content\": patch[\"content\"]\n",
    "    })\n",
    "\n",
    "print(structured_patches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0e98dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved successfully.\n"
     ]
    }
   ],
   "source": [
    "with open(\"diablo_iv_patches_structured.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(structured_patches, f, indent=4)\n",
    "\n",
    "print(\"Saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7479fba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>build</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.5.0</td>\n",
       "      <td>69713</td>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>Note:\\nAdditional changes made since the PTR’s...</td>\n",
       "      <td>10650</td>\n",
       "      <td>66650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.5.1</td>\n",
       "      <td>69864</td>\n",
       "      <td>2025-12-18</td>\n",
       "      <td>Game Updates\\nBase Game\\nUpdated descriptions ...</td>\n",
       "      <td>661</td>\n",
       "      <td>4061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.5.2</td>\n",
       "      <td>70156</td>\n",
       "      <td>2026-01-12</td>\n",
       "      <td>Game Updates\\nBase Game\\nThe Tower and Leaderb...</td>\n",
       "      <td>1116</td>\n",
       "      <td>6674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.5.3</td>\n",
       "      <td>70356</td>\n",
       "      <td>2026-01-28</td>\n",
       "      <td>Bug Fixes\\nFixed an issue where the Executione...</td>\n",
       "      <td>194</td>\n",
       "      <td>1157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  version  build       date  \\\n",
       "3   2.5.0  69713 2025-12-11   \n",
       "2   2.5.1  69864 2025-12-18   \n",
       "1   2.5.2  70156 2026-01-12   \n",
       "0   2.5.3  70356 2026-01-28   \n",
       "\n",
       "                                             content  word_count  char_count  \n",
       "3  Note:\\nAdditional changes made since the PTR’s...       10650       66650  \n",
       "2  Game Updates\\nBase Game\\nUpdated descriptions ...         661        4061  \n",
       "1  Game Updates\\nBase Game\\nThe Tower and Leaderb...        1116        6674  \n",
       "0  Bug Fixes\\nFixed an issue where the Executione...         194        1157  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(structured_patches)\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"word_count\"] = df[\"content\"].apply(lambda x: len(x.split()))\n",
    "df[\"char_count\"] = df[\"content\"].apply(len)\n",
    "\n",
    "df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "686ba0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>date</th>\n",
       "      <th>word_count</th>\n",
       "      <th>patch_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.5.0</td>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>10650</td>\n",
       "      <td>Major Update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.5.1</td>\n",
       "      <td>2025-12-18</td>\n",
       "      <td>661</td>\n",
       "      <td>Minor / Hotfix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.5.2</td>\n",
       "      <td>2026-01-12</td>\n",
       "      <td>1116</td>\n",
       "      <td>Mid Update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.5.3</td>\n",
       "      <td>2026-01-28</td>\n",
       "      <td>194</td>\n",
       "      <td>Minor / Hotfix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  version       date  word_count      patch_type\n",
       "3   2.5.0 2025-12-11       10650    Major Update\n",
       "2   2.5.1 2025-12-18         661  Minor / Hotfix\n",
       "1   2.5.2 2026-01-12        1116      Mid Update\n",
       "0   2.5.3 2026-01-28         194  Minor / Hotfix"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_patch(word_count):\n",
    "    if word_count > 5000:\n",
    "        return \"Major Update\"\n",
    "    elif word_count > 1000:\n",
    "        return \"Mid Update\"\n",
    "    else:\n",
    "        return \"Minor / Hotfix\"\n",
    "\n",
    "df[\"patch_type\"] = df[\"word_count\"].apply(classify_patch)\n",
    "\n",
    "df[[\"version\", \"date\", \"word_count\", \"patch_type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac18f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2026.1.15-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\renai\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.9 MB/s  0:00:00\n",
      "Downloading regex-2026.1.15-cp313-cp313-win_amd64.whl (277 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   ---------------------------------------- 5/5 [nltk]\n",
      "\n",
      "Successfully installed click-8.3.1 joblib-1.5.3 nltk-3.9.2 regex-2026.1.15 tqdm-4.67.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/nltk/\n",
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /packages/60/90/81ac364ef94209c100e12579629dc92bf7a709a84af32f8c551b02c07e94/nltk-3.9.2-py3-none-any.whl.metadata\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bad0260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\renai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\renai/nltk_data'\n    - 'c:\\\\Users\\\\renai\\\\miniconda3\\\\envs\\\\h501-gutenberg\\\\nltk_data'\n    - 'c:\\\\Users\\\\renai\\\\miniconda3\\\\envs\\\\h501-gutenberg\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\renai\\\\miniconda3\\\\envs\\\\h501-gutenberg\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\renai\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m MAX_WORDS = \u001b[32m400\u001b[39m  \u001b[38;5;66;03m# Ideal for embeddings / transformers\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m patches:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     sentences = \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     current_chunk = []\n\u001b[32m     15\u001b[39m     current_word_count = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\renai\\miniconda3\\envs\\h501-gutenberg\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\renai\\miniconda3\\envs\\h501-gutenberg\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\renai\\miniconda3\\envs\\h501-gutenberg\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\renai\\miniconda3\\envs\\h501-gutenberg\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\renai\\miniconda3\\envs\\h501-gutenberg\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\renai/nltk_data'\n    - 'c:\\\\Users\\\\renai\\\\miniconda3\\\\envs\\\\h501-gutenberg\\\\nltk_data'\n    - 'c:\\\\Users\\\\renai\\\\miniconda3\\\\envs\\\\h501-gutenberg\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\renai\\\\miniconda3\\\\envs\\\\h501-gutenberg\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\renai\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Load your JSON\n",
    "with open(\"diablo_iv_patches_structured.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    patches = json.load(f)\n",
    "\n",
    "chunks = []\n",
    "chunk_id = 0\n",
    "MAX_WORDS = 400  # Ideal for embeddings / transformers\n",
    "\n",
    "for patch in patches:\n",
    "    sentences = sent_tokenize(patch[\"content\"])\n",
    "    \n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        word_count = len(sentence.split())\n",
    "        \n",
    "        if current_word_count + word_count > MAX_WORDS:\n",
    "            # Save chunk\n",
    "            chunks.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"version\": patch[\"version\"],\n",
    "                \"build\": patch[\"build\"],\n",
    "                \"date\": patch[\"date\"],\n",
    "                \"chunk_text\": \" \".join(current_chunk),\n",
    "                \"word_count\": current_word_count\n",
    "            })\n",
    "            \n",
    "            chunk_id += 1\n",
    "            current_chunk = []\n",
    "            current_word_count = 0\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_word_count += word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2255fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h501-gutenberg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
